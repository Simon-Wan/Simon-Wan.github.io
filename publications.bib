
@misc{noauthor_quick_start_guide_nodate,
	title = {quick\_start\_guide [{Zotero} {Documentation}]},
	url = {https://www.zotero.org/support/quick_start_guide},
	urldate = {2024-09-18},
	file = {quick_start_guide [Zotero Documentation]:/Users/Roy/Zotero/storage/8ESGUPRZ/quick_start_guide.html:text/html},
}

@misc{sun_planning_2011,
	title = {Planning to {Be} {Surprised}: {Optimal} {Bayesian} {Exploration} in {Dynamic} {Environments}},
	shorttitle = {Planning to {Be} {Surprised}},
	url = {http://arxiv.org/abs/1103.5708},
	abstract = {To maximize its success, an AGI typically needs to explore its initially unknown world. Is there an optimal way of doing so? Here we derive an aﬃrmative answer for a broad class of environments.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Sun, Yi and Gomez, Faustino and Schmidhuber, Juergen},
	month = mar,
	year = {2011},
	note = {arXiv:1103.5708 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning},
	file = {PDF:/Users/Roy/Zotero/storage/JZBG8ZS6/Sun 等 - 2011 - Planning to Be Surprised Optimal Bayesian Exploration in Dynamic Environments.pdf:application/pdf},
}

@inproceedings{schmidhuber_curious_1991,
	title = {Curious model-building control systems},
	url = {https://ieeexplore.ieee.org/document/170605/?arnumber=170605},
	doi = {10.1109/IJCNN.1991.170605},
	abstract = {A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a four-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an artificial nondeterministic environment demonstrates that the system can be superior to previous model-building control systems, which do not address the problem of modeling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model.{\textless}{\textgreater}},
	urldate = {2024-09-18},
	booktitle = {[{Proceedings}] 1991 {IEEE} {International} {Joint} {Conference} on {Neural} {Networks}},
	author = {Schmidhuber, J.},
	month = nov,
	year = {1991},
	keywords = {Adaptive control, Computer architecture, Computer networks, Computer science, Control system synthesis, Error correction, Learning, Manipulator dynamics, Predictive models, Programmable control},
	pages = {1458--1463 vol.2},
	file = {IEEE Xplore Abstract Record:/Users/Roy/Zotero/storage/WIMGEAQE/170605.html:text/html;IEEE Xplore Full Text PDF:/Users/Roy/Zotero/storage/528NQL2X/Schmidhuber - 1991 - Curious model-building control systems.pdf:application/pdf},
}

@misc{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	url = {http://arxiv.org/abs/2203.02155},
	abstract = {Making language models bigger does not inherently make them better at following a user’s intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by ﬁne-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through the OpenAI API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to ﬁne-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further ﬁne-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that ﬁne-tuning with human feedback is a promising direction for aligning language models with human intent.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
	month = mar,
	year = {2022},
	note = {arXiv:2203.02155 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/Roy/Zotero/storage/DTNIVNUA/Ouyang 等 - 2022 - Training language models to follow instructions with human feedback.pdf:application/pdf},
}

@misc{houthooft_vime_2017,
	title = {{VIME}: {Variational} {Information} {Maximizing} {Exploration}},
	shorttitle = {{VIME}},
	url = {http://arxiv.org/abs/1605.09674},
	abstract = {Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as  -greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent’s belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efﬁciently handles continuous state and action spaces. VIME modiﬁes the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves signiﬁcantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Houthooft, Rein and Chen, Xi and Duan, Yan and Schulman, John and De Turck, Filip and Abbeel, Pieter},
	month = jan,
	year = {2017},
	note = {arXiv:1605.09674 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Robotics},
	annote = {Comment: Published in Advances in Neural Information Processing Systems 29 (NIPS), pages 1109-1117},
	file = {PDF:/Users/Roy/Zotero/storage/RUEQ8MMI/Houthooft 等 - 2017 - VIME Variational Information Maximizing Exploration.pdf:application/pdf},
}

@misc{jaques_human-centric_2020,
	title = {Human-centric {Dialog} {Training} via {Offline} {Reinforcement} {Learning}},
	url = {http://arxiv.org/abs/2010.05848},
	abstract = {How can we train a dialog model to produce better conversations by learning from human feedback, without the risk of humans teaching it harmful chat behaviors? We start by hosting models online, and gather human feedback from real-time, open-ended conversations, which we then use to train and improve the models using ofﬂine reinforcement learning (RL). We identify implicit conversational cues including language similarity, elicitation of laughter, sentiment, and more, which indicate positive human feedback, and embed these in multiple reward functions. A wellknown challenge is that learning an RL policy in an ofﬂine setting usually fails due to the lack of ability to explore and the tendency to make over-optimistic estimates of future reward. These problems become even harder when using RL for language models, which can easily have a 20,000 action vocabulary and many possible reward functions. We solve the challenge by developing a novel class of ofﬂine RL algorithms. These algorithms use KL-control to penalize divergence from a pretrained prior language model, and use a new strategy to make the algorithm pessimistic, instead of optimistic, in the face of uncertainty. We test the resulting dialog model with ratings from 80 users in an open-domain setting and ﬁnd it achieves signiﬁcant improvements over existing deep ofﬂine RL approaches. The novel ofﬂine RL method is viable for improving any existing generative dialog model using a static dataset of human feedback.},
	language = {en},
	urldate = {2024-09-18},
	publisher = {arXiv},
	author = {Jaques, Natasha and Shen, Judy Hanwen and Ghandeharioun, Asma and Ferguson, Craig and Lapedriza, Agata and Jones, Noah and Gu, Shixiang Shane and Picard, Rosalind},
	month = oct,
	year = {2020},
	note = {arXiv:2010.05848 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: To appear in EMNLP 2020 (long paper)},
	file = {PDF:/Users/Roy/Zotero/storage/9DXEDHDS/Jaques 等 - 2020 - Human-centric Dialog Training via Offline Reinforcement Learning.pdf:application/pdf},
}

@article{speer_hyperscanning_2024,
	title = {Hyperscanning shows friends explore and strangers converge in conversation},
	volume = {15},
	copyright = {2024 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-51990-7},
	doi = {10.1038/s41467-024-51990-7},
	abstract = {During conversation, people often endeavor to convey information in an understandable way (finding common ground) while also sharing novel or surprising information (exploring new ground). Here, we test how friends and strangers balance these two strategies to connect with each other. Using fMRI hyperscanning, we measure a preference for common ground as convergence over time and exploring new ground as divergence over time by tracking dyads’ neural and linguistic trajectories over the course of semi-structured intimacy-building conversations. In our study, 60 dyads (30 friend dyads) engaged in a real-time conversation with discrete prompts and demarcated turns. Our analyses reveal that friends diverge neurally and linguistically: their neural patterns become more dissimilar over time and they explore more diverse topics. In contrast, strangers converge: neural patterns and language become more similar over time. The more a conversation between strangers resembles the exploratory conversations of friends, the more they enjoy it. Our results highlight exploring new ground as a strategy for a successful conversation.},
	language = {en},
	number = {1},
	urldate = {2024-09-18},
	journal = {Nature Communications},
	author = {Speer, Sebastian P. H. and Mwilambwe-Tshilobo, Laetitia and Tsoi, Lily and Burns, Shannon M. and Falk, Emily B. and Tamir, Diana I.},
	month = sep,
	year = {2024},
	note = {Publisher: Nature Publishing Group},
	keywords = {Human behaviour, Social neuroscience},
	pages = {7781},
	file = {Full Text PDF:/Users/Roy/Zotero/storage/4V22BASF/Speer 等 - 2024 - Hyperscanning shows friends explore and strangers converge in conversation.pdf:application/pdf},
}

@misc{hu_off-belief_2021,
	title = {Off-{Belief} {Learning}},
	url = {http://arxiv.org/abs/2103.04000},
	abstract = {The standard problem setting in Dec-POMDPs is self-play, where the goal is to ﬁnd a set of policies that play optimally together. Policies learned through self-play may adopt arbitrary conventions and implicitly rely on multi-step reasoning based on fragile assumptions about other agents’ actions and thus fail when paired with humans or independently trained agents at test time. To address this, we present off-belief learning (OBL). At each timestep OBL agents follow a policy π1 that is optimized assuming past actions were taken by a given, ﬁxed policy (π0), but assuming that future actions will be taken by π1. When π0 is uniform random, OBL converges to an optimal policy that does not rely on inferences based on other agents’ behavior (an optimal grounded policy). OBL can be iterated in a hierarchy, where the optimal policy from one level becomes the input to the next, thereby introducing multi-level cognitive reasoning in a controlled manner. Unlike existing approaches, which may converge to any equilibrium policy, OBL converges to a unique policy, making it suitable for zero-shot coordination (ZSC). OBL can be scaled to high-dimensional settings with a ﬁctitious transition mechanism and shows strong performance in both a toy-setting and the benchmark human-AI \& ZSC problem Hanabi.},
	language = {en},
	urldate = {2024-09-19},
	publisher = {arXiv},
	author = {Hu, Hengyuan and Lerer, Adam and Cui, Brandon and Wu, David and Pineda, Luis and Brown, Noam and Foerster, Jakob},
	month = aug,
	year = {2021},
	note = {arXiv:2103.04000 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {PDF:/Users/Roy/Zotero/storage/9QJWW4QR/Hu 等 - 2021 - Off-Belief Learning.pdf:application/pdf},
}

@inproceedings{feng_mmdialog_2023,
	address = {Toronto, Canada},
	title = {{MMDialog}: {A} {Large}-scale {Multi}-turn {Dialogue} {Dataset} {Towards} {Multi}-modal {Open}-domain {Conversation}},
	shorttitle = {{MMDialog}},
	url = {https://aclanthology.org/2023.acl-long.405},
	doi = {10.18653/v1/2023.acl-long.405},
	abstract = {Responding with multi-modal content has been recognized as an essential capability for an intelligent conversational agent. In this paper, we introduce the MMDialog dataset to facilitate multi-modal conversation better. MMDialog is composed of a curated set of 1.08 million real-world dialogues with 1.53 million unique images across 4,184 topics. MMDialog has two main and unique advantages. First, it is the largest multi-modal conversation dataset by the number of dialogues by 88x. Second, it contains massive topics to generalize the open domain. To build an engaging dialogue system with this dataset, we propose and normalize two response prediction tasks based on retrieval and generative scenarios. In addition, we build two baselines for the above tasks with state-of-the-art techniques and report their experimental performance. We also propose a novel evaluation metric MM-Relevance to measure the multi-modal responses. Our dataset is available in https://github.com/victorsungo/MMDialog.},
	urldate = {2024-09-23},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Feng, Jiazhan and Sun, Qingfeng and Xu, Can and Zhao, Pu and Yang, Yaming and Tao, Chongyang and Zhao, Dongyan and Lin, Qingwei},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {7348--7363},
	file = {Full Text PDF:/Users/Roy/Zotero/storage/TZ25QELT/Feng 等 - 2023 - MMDialog A Large-scale Multi-turn Dialogue Dataset Towards Multi-modal Open-domain Conversation.pdf:application/pdf},
}

@misc{li_dailydialog_2017,
	title = {{DailyDialog}: {A} {Manually} {Labelled} {Multi}-turn {Dialogue} {Dataset}},
	shorttitle = {{DailyDialog}},
	url = {http://arxiv.org/abs/1710.03957},
	abstract = {We develop a high-quality multi-turn dialog dataset, DailyDialog, which is intriguing in several aspects. The language is human-written and less noisy. The dialogues in the dataset reﬂect our daily communication way and cover various topics about our daily life. We also manually label the developed dataset with communication intention and emotion information. Then, we evaluate existing approaches on DailyDialog dataset and hope it beneﬁt the research ﬁeld of dialog systems1.},
	language = {en},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Li, Yanran and Su, Hui and Shen, Xiaoyu and Li, Wenjie and Cao, Ziqiang and Niu, Shuzi},
	month = oct,
	year = {2017},
	note = {arXiv:1710.03957 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: accepted by IJCNLP 2017},
	file = {PDF:/Users/Roy/Zotero/storage/JUNFD9DY/Li 等 - 2017 - DailyDialog A Manually Labelled Multi-turn Dialogue Dataset.pdf:application/pdf},
}

@misc{qian_pchatbot_2021,
	title = {Pchatbot: {A} {Large}-{Scale} {Dataset} for {Personalized} {Chatbot}},
	shorttitle = {Pchatbot},
	url = {http://arxiv.org/abs/2009.13284},
	abstract = {Natural language dialogue systems raise great attention recently. As many dialogue models are data-driven, high-quality datasets are essential to these systems. In this paper, we introduce Pchatbot, a large-scale dialogue dataset that contains two subsets collected from Weibo and Judicial forums respectively. To adapt the raw dataset to dialogue systems, we elaborately normalize the raw dataset via processes such as anonymization, deduplication, segmentation, and filtering. The scale of Pchatbot is significantly larger than existing Chinese datasets, which might benefit the data-driven models. Besides, current dialogue datasets for personalized chatbot usually contain several persona sentences or attributes. Different from existing datasets, Pchatbot provides anonymized user IDs and timestamps for both posts and responses. This enables the development of personalized dialogue models that directly learn implicit user personality from the user’s dialogue history. Our preliminary experimental study benchmarks several state-of-the-art dialogue models to provide a comparison for future work. The dataset can be publicly accessed at Github: https://github.com/qhjqhj00/Pchatbot.},
	language = {en},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Qian, Hongjin and Li, Xiaohe and Zhong, Hanxun and Guo, Yu and Ma, Yueyuan and Zhu, Yutao and Liu, Zhanliang and Dou, Zhicheng and Wen, Ji-Rong},
	month = may,
	year = {2021},
	note = {arXiv:2009.13284 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Camera-ready version, SIGIR 2021 (Resource Track), the dataset and codes are available at https://github.com/qhjqhj00/Pchatbot},
	file = {PDF:/Users/Roy/Zotero/storage/MKXVRV4F/Qian 等 - 2021 - Pchatbot A Large-Scale Dataset for Personalized Chatbot.pdf:application/pdf},
}

@misc{zheng_personalized_2020,
	title = {Personalized {Dialogue} {Generation} with {Diversified} {Traits}},
	url = {http://arxiv.org/abs/1901.09672},
	abstract = {Endowing a dialogue system with particular personality traits is essential to deliver more human-like conversations. However, due to the challenge of embodying personality via language expression and the lack of large-scale persona-labeled dialogue data, this research problem is still far from well-studied. In this paper, we investigate the problem of incorporating explicit personality traits in dialogue generation to deliver personalized dialogues.},
	language = {en},
	urldate = {2024-09-23},
	publisher = {arXiv},
	author = {Zheng, Yinhe and Chen, Guanyi and Huang, Minlie and Liu, Song and Zhu, Xuan},
	month = jan,
	year = {2020},
	note = {arXiv:1901.09672 [cs]},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Please contact [zhengyinhe1 at 163 dot com] for the PersonalDialog dataset},
	file = {PDF:/Users/Roy/Zotero/storage/7WKFBN4V/Zheng 等 - 2020 - Personalized Dialogue Generation with Diversified Traits.pdf:application/pdf},
}

@misc{abdulhai_lmrl_2023,
	title = {{LMRL} {Gym}: {Benchmarks} for {Multi}-{Turn} {Reinforcement} {Learning} with {Language} {Models}},
	shorttitle = {{LMRL} {Gym}},
	url = {http://arxiv.org/abs/2311.18232},
	abstract = {Large language models (LLMs) provide excellent text-generation capabilities, but standard prompting and generation methods generally do not lead to intentional or goal-directed agents and might necessitate considerable prompt tuning. This becomes particularly apparent in multi-turn conversations: even the best current LLMs rarely ask clarifying questions, engage in explicit information gathering, or take actions now that lead to better decisions after multiple turns. Reinforcement learning has the potential to leverage the powerful modeling capabilities of LLMs, as well as their internal representation of textual interactions, to create capable goal-directed language agents. This can enable intentional and temporally extended interactions, such as with humans, through coordinated persuasion and carefully crafted questions, or in goal-directed play through text games to bring about desired final outcomes. However, enabling this requires the community to develop stable and reliable reinforcement learning algorithms that can effectively train LLMs. Developing such algorithms requires tasks that can gauge progress on algorithm design, provide accessible and reproducible evaluations for multi-turn interactions, and cover a range of task properties and challenges in improving reinforcement learning algorithms. Our paper introduces the LMRL-Gym benchmark for evaluating multi-turn RL for LLMs, together with an open-source research framework containing a basic toolkit for getting started on multi-turn RL with offline value-based and policy-based RL methods. Our benchmark consists of 8 different language tasks, which require multiple rounds of language interaction and cover a range of tasks in open-ended dialogue and text games.},
	language = {en},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Abdulhai, Marwa and White, Isadora and Snell, Charlie and Sun, Charles and Hong, Joey and Zhai, Yuexiang and Xu, Kelvin and Levine, Sergey},
	month = nov,
	year = {2023},
	note = {arXiv:2311.18232 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/Roy/Zotero/storage/IV2H2RSA/Abdulhai 等 - 2023 - LMRL Gym Benchmarks for Multi-Turn Reinforcement Learning with Language Models.pdf:application/pdf},
}

@misc{ghandeharioun_approximating_2019,
	title = {Approximating {Interactive} {Human} {Evaluation} with {Self}-{Play} for {Open}-{Domain} {Dialog} {Systems}},
	url = {http://arxiv.org/abs/1906.09308},
	abstract = {Building an open-domain conversational agent is a challenging problem. Current evaluation methods, mostly post-hoc judgments of static conversation, do not capture conversation quality in a realistic interactive context. In this paper, we investigate interactive human evaluation and provide evidence for its necessity; we then introduce a novel, model-agnostic, and dataset-agnostic method to approximate it. In particular, we propose a self-play scenario where the dialog system talks to itself and we calculate a combination of proxies such as sentiment and semantic coherence on the conversation trajectory. We show that this metric is capable of capturing the human-rated quality of a dialog model better than any automated metric known to-date, achieving a signiﬁcant Pearson correlation (r {\textgreater} .7, p {\textless} .05). To investigate the strengths of this novel metric and interactive evaluation in comparison to state-of-the-art metrics and human evaluation of static conversations, we perform extended experiments with a set of models, including several that make novel improvements to recent hierarchical dialog generation architectures through sentiment and semantic knowledge distillation on the utterance level. Finally, we open-source the interactive evaluation platform we built and the dataset we collected to allow researchers to efﬁciently deploy and evaluate dialog models.},
	language = {en},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Ghandeharioun, Asma and Shen, Judy Hanwen and Jaques, Natasha and Ferguson, Craig and Jones, Noah and Lapedriza, Agata and Picard, Rosalind},
	month = nov,
	year = {2019},
	note = {arXiv:1906.09308 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Comment: 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
	file = {PDF:/Users/Roy/Zotero/storage/DWSQT8PE/Ghandeharioun 等 - 2019 - Approximating Interactive Human Evaluation with Self-Play for Open-Domain Dialog Systems.pdf:application/pdf},
}

@misc{li_mediq_2024,
	title = {{MEDIQ}: {Question}-{Asking} {LLMs} for {Adaptive} and {Reliable} {Clinical} {Reasoning}},
	shorttitle = {{MEDIQ}},
	url = {http://arxiv.org/abs/2406.00922},
	abstract = {In high-stakes domains like clinical reasoning, AI assistants powered by large language models (LLMs) are yet to be reliable and safe. We identify a key obstacle towards reliability: existing LLMs are trained to answer any question, even with incomplete context in the prompt or insufficient parametric knowledge. We propose to change this paradigm to develop more careful LLMs that ask follow-up questions to gather necessary and sufficient information and respond reliably. We introduce MEDIQ, a framework to simulate realistic clinical interactions, which incorporates a Patient System and an adaptive Expert System. The Patient may provide incomplete information in the beginning; the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details from the Patient via follow-up questions. To evaluate MEDIQ, we convert MEDQA and CRAFT-MD—medical benchmarks for diagnostic question answering—into an interactive setup. We develop a reliable Patient system and prototype several Expert systems, first showing that directly prompting state-of-the-art LLMs to ask questions degrades the quality of clinical reasoning, indicating that adapting LLMs to interactive information-seeking settings is nontrivial. We then augment the Expert with a novel abstention module to better estimate model confidence and decide whether to ask more questions, thereby improving diagnostic accuracy by 22.3\%; however, performance still lags compared to an (unrealistic in practice) upper bound when full information is given upfront. Further analyses reveal that interactive performance can be improved by filtering irrelevant contexts and reformatting conversations. Overall, our paper introduces a novel problem towards LLM reliability, a novel MEDIQ framework, and highlights important future directions to extend the information-seeking abilities of LLM assistants in critical domains.},
	language = {en},
	urldate = {2024-09-26},
	publisher = {arXiv},
	author = {Li, Shuyue Stella and Balachandran, Vidhisha and Feng, Shangbin and Ilgen, Jonathan and Pierson, Emma and Koh, Pang Wei and Tsvetkov, Yulia},
	month = jun,
	year = {2024},
	note = {arXiv:2406.00922 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: 29 pages, 12 figures},
	file = {PDF:/Users/Roy/Zotero/storage/NWLRTUNR/Li 等 - 2024 - MEDIQ Question-Asking LLMs for Adaptive and Reliable Clinical Reasoning.pdf:application/pdf},
}

@misc{munos_nash_2024,
	title = {Nash {Learning} from {Human} {Feedback}},
	url = {http://arxiv.org/abs/2312.00886},
	abstract = {Reinforcement learning from human feedback (RLHF) has emerged as the main paradigm for aligning large language models (LLMs) with human preferences. Traditionally, RLHF involves the initial step of learning a reward model from pairwise human feedback, i.e., expressed as preferences between pairs of text generations. Subsequently, the LLM’s policy is fine-tuned to maximize the reward through a reinforcement learning algorithm. In this study, we introduce an alternative pipeline for the fine-tuning of LLMs using pairwise human feedback. Our approach entails the initial learning of a pairwise preference model, which is conditioned on two inputs (instead of a single input in the case of a reward model) given a prompt, followed by the pursuit of a policy that consistently generates responses preferred over those generated by any competing policy, thus defining the Nash equilibrium of this preference model. We term this approach Nash learning from human feedback (NLHF). In the context of a tabular policy representation, we present a novel algorithmic solution, NashMD, founded on the principles of mirror descent. This algorithm produces a sequence of policies, with the last iteration converging to the regularized Nash equilibrium. Additionally, we explore parametric representations of policies and introduce gradient descent algorithms for deeplearning architectures. We illustrate the effectiveness of our approach by presenting experimental results on a text summarization task. We believe *Equal contribution 1Google DeepMind 2ENSAE Paris †Now at Cohere. Correspondence to: Remi Munos {\textless}remi.munos@inria.fr{\textgreater}, Michal Valko {\textless}michal.valko@inria.fr{\textgreater}, Daniele Calandriello {\textless}dcalandriello@google.com{\textgreater}, Bilal Piot {\textless}piot@google.com{\textgreater}.},
	language = {en},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Munos, Rémi and Valko, Michal and Calandriello, Daniele and Azar, Mohammad Gheshlaghi and Rowland, Mark and Guo, Zhaohan Daniel and Tang, Yunhao and Geist, Matthieu and Mesnard, Thomas and Michi, Andrea and Selvi, Marco and Girgin, Sertan and Momchev, Nikola and Bachem, Olivier and Mankowitz, Daniel J. and Precup, Doina and Piot, Bilal},
	month = jun,
	year = {2024},
	note = {arXiv:2312.00886 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Statistics - Machine Learning, Computer Science - Machine Learning, Computer Science - Computer Science and Game Theory, Computer Science - Multiagent Systems},
	file = {PDF:/Users/Roy/Zotero/storage/MSW4BZ7B/Munos 等 - 2024 - Nash Learning from Human Feedback.pdf:application/pdf},
}

@misc{shani_multi-turn_2024,
	title = {Multi-turn {Reinforcement} {Learning} from {Preference} {Human} {Feedback}},
	url = {http://arxiv.org/abs/2405.14655},
	abstract = {Reinforcement Learning from Human Feedback (RLHF) has become the standard approach for aligning Large Language Models (LLMs) with human preferences, allowing LLMs to demonstrate remarkable abilities in various tasks. Existing methods work by emulating the preferences at the single decision (turn) level, limiting their capabilities in settings that require planning or multi-turn interactions to achieve a long-term goal. In this paper, we address this issue by developing novel methods for Reinforcement Learning (RL) from preference feedback between two full multi-turn conversations. In the tabular setting, we present a novel mirrordescent-based policy optimization algorithm for the general multi-turn preferencebased RL problem, and prove its convergence to Nash equilibrium. To evaluate performance, we create a new environment, Education Dialogue, where a teacher agent guides a student in learning a random topic, and show that a deep RL variant of our algorithm outperforms RLHF baselines. Finally, we show that in an environment with explicit rewards, our algorithm recovers the same performance as a reward-based RL baseline, despite relying solely on a weaker preference signal.},
	language = {en},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Shani, Lior and Rosenberg, Aviv and Cassel, Asaf and Lang, Oran and Calandriello, Daniele and Zipori, Avital and Noga, Hila and Keller, Orgad and Piot, Bilal and Szpektor, Idan and Hassidim, Avinatan and Matias, Yossi and Munos, Rémi},
	month = may,
	year = {2024},
	note = {arXiv:2405.14655 [cs]},
	keywords = {Computer Science - Machine Learning},
	file = {PDF:/Users/Roy/Zotero/storage/F97RJCW3/Shani 等 - 2024 - Multi-turn Reinforcement Learning from Preference Human Feedback.pdf:application/pdf},
}

@misc{qu_recursive_2024,
	title = {Recursive {Introspection}: {Teaching} {Language} {Model} {Agents} {How} to {Self}-{Improve}},
	shorttitle = {Recursive {Introspection}},
	url = {http://arxiv.org/abs/2407.18219},
	abstract = {A central piece in enabling intelligent agentic behavior in foundation models is to make them capable of introspecting upon their behavior, reasoning, and correcting their mistakes as more computation or interaction is available. Even the strongest proprietary large language models (LLMs) do not quite exhibit the ability of continually improving their responses sequentially, even in scenarios where they are explicitly told that they are making a mistake. In this paper, we develop RISE: Recursive IntroSpEction, an approach for fine-tuning LLMs to introduce this capability, despite prior work hypothesizing that this capability may not be possible to attain. Our approach prescribes an iterative fine-tuning procedure, which attempts to teach the model how to alter its response after having executed previously unsuccessful attempts to solve a hard test-time problem, with optionally additional environment feedback. RISE poses fine-tuning for a single-turn prompt as solving a multi-turn Markov decision process (MDP), where the initial state is the prompt. Inspired by principles in online imitation learning and reinforcement learning, we propose strategies for multi-turn data collection and training so as to imbue an LLM with the capability to recursively detect and correct its previous mistakes in subsequent iterations. Our experiments show that RISE enables Llama2, Llama3, and Mistral models to improve themselves with more turns on math reasoning tasks, outperforming several single-turn strategies given an equal amount of inference-time computation. We also find that RISE scales well, often attaining larger benefits with more capable models. Our analysis shows that RISE makes meaningful improvements to responses to arrive at the correct solution for challenging prompts, without disrupting one-turn abilities as a result of expressing more complex distributions.},
	language = {en},
	urldate = {2024-09-30},
	publisher = {arXiv},
	author = {Qu, Yuxiao and Zhang, Tianjun and Garg, Naman and Kumar, Aviral},
	month = jul,
	year = {2024},
	note = {arXiv:2407.18219 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {PDF:/Users/Roy/Zotero/storage/XGTL6RJ4/Qu 等 - 2024 - Recursive Introspection Teaching Language Model Agents How to Self-Improve.pdf:application/pdf},
}

@misc{wan_handmethat_2023,
	title = {{HandMeThat}: {Human}-{Robot} {Communication} in {Physical} and {Social} {Environments}},
	shorttitle = {{HandMeThat}},
	url = {http://arxiv.org/abs/2310.03779},
	abstract = {We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social environments. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human actions and goals) information. HandMeThat contains 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction. In this paper, we present a textual interface for our benchmark, where the robot interacts with a virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting significant room for future work on physical and social human-robot communications and interactions.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Wan, Yanming and Mao, Jiayuan and Tenenbaum, Joshua B.},
	month = oct,
	year = {2023},
	note = {arXiv:2310.03779},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:/Users/Roy/Zotero/storage/6UZWIZLS/Wan 等 - 2023 - HandMeThat Human-Robot Communication in Physical and Social Environments.pdf:application/pdf;Snapshot:/Users/Roy/Zotero/storage/M3XK9EYM/2310.html:text/html},
}

@misc{poddar_personalizing_2024,
	title = {Personalizing {Reinforcement} {Learning} from {Human} {Feedback} with {Variational} {Preference} {Learning}},
	url = {http://arxiv.org/abs/2408.10075},
	abstract = {Reinforcement Learning from Human Feedback (RLHF) is a powerful paradigm for aligning foundation models to human values and preferences. However, current RLHF techniques cannot account for the naturally occurring differences in individual human preferences across a diverse population. When these differences arise, traditional RLHF frameworks simply average over them, leading to inaccurate rewards and poor performance for individual subgroups. To address the need for pluralistic alignment, we develop a class of multimodal RLHF methods. Our proposed techniques are based on a latent variable formulation - inferring a novel user-specific latent and learning reward models and policies conditioned on this latent without additional user-specific data. While conceptually simple, we show that in practice, this reward modeling requires careful algorithmic considerations around model architecture and reward scaling. To empirically validate our proposed technique, we first show that it can provide a way to combat underspecification in simulated control problems, inferring and optimizing user-specific reward functions. Next, we conduct experiments on pluralistic language datasets representing diverse user preferences and demonstrate improved reward function accuracy. We additionally show the benefits of this probabilistic framework in terms of measuring uncertainty, and actively learning user preferences. This work enables learning from diverse populations of users with divergent preferences, an important challenge that naturally occurs in problems from robot learning to foundation model alignment.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Poddar, Sriyash and Wan, Yanming and Ivison, Hamish and Gupta, Abhishek and Jaques, Natasha},
	month = aug,
	year = {2024},
	note = {arXiv:2408.10075 
version: 1},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning, Computer Science - Robotics},
	file = {Preprint PDF:/Users/Roy/Zotero/storage/4RALBTLQ/Poddar 等 - 2024 - Personalizing Reinforcement Learning from Human Feedback with Variational Preference Learning.pdf:application/pdf;Snapshot:/Users/Roy/Zotero/storage/8BQ4BCGV/2408.html:text/html},
}

@misc{wan_infer_2024,
	title = {Infer {Human}'s {Intentions} {Before} {Following} {Natural} {Language} {Instructions}},
	url = {http://arxiv.org/abs/2409.18073},
	abstract = {For AI agents to be helpful to humans, they should be able to follow natural language instructions to complete everyday cooperative tasks in human environments. However, real human instructions inherently possess ambiguity, because the human speakers assume sufficient prior knowledge about their hidden goals and intentions. Standard language grounding and planning methods fail to address such ambiguities because they do not model human internal goals as additional partially observable factors in the environment. We propose a new framework, Follow Instructions with Social and Embodied Reasoning (FISER), aiming for better natural language instruction following in collaborative embodied tasks. Our framework makes explicit inferences about human goals and intentions as intermediate reasoning steps. We implement a set of Transformer-based models and evaluate them over a challenging benchmark, HandMeThat. We empirically demonstrate that using social reasoning to explicitly infer human intentions before making action plans surpasses purely end-to-end approaches. We also compare our implementation with strong baselines, including Chain of Thought prompting on the largest available pre-trained language models, and find that FISER provides better performance on the embodied social reasoning tasks under investigation, reaching the state-of-the-art on HandMeThat.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Wan, Yanming and Wu, Yue and Wang, Yiping and Mao, Jiayuan and Jaques, Natasha},
	month = sep,
	year = {2024},
	note = {arXiv:2409.18073},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Preprint PDF:/Users/Roy/Zotero/storage/PS2KFCYW/Wan 等 - 2024 - Infer Human's Intentions Before Following Natural Language Instructions.pdf:application/pdf;Snapshot:/Users/Roy/Zotero/storage/F2M6527D/2409.html:text/html},
}

@misc{yang_toward_2024,
	title = {Toward a {More} {Complete} {OMR} {Solution}},
	url = {http://arxiv.org/abs/2409.00316},
	abstract = {Optical music recognition (OMR) aims to convert music notation into digital formats. One approach to tackle OMR is through a multi-stage pipeline, where the system first detects visual music notation elements in the image (object detection) and then assembles them into a music notation (notation assembly). Most previous work on notation assembly unrealistically assumes perfect object detection. In this study, we focus on the MUSCIMA++ v2.0 dataset, which represents musical notation as a graph with pairwise relationships among detected music objects, and we consider both stages together. First, we introduce a music object detector based on YOLOv8, which improves detection performance. Second, we introduce a supervised training pipeline that completes the notation assembly stage based on detection output. We find that this model is able to outperform existing models trained on perfect detection output, showing the benefit of considering the detection and assembly stages in a more holistic way. These findings, together with our novel evaluation metric, are important steps toward a more complete OMR solution.},
	urldate = {2024-10-27},
	publisher = {arXiv},
	author = {Yang, Guang and Zhang, Muru and Qiu, Lin and Wan, Yanming and Smith, Noah A.},
	month = aug,
	year = {2024},
	note = {arXiv:2409.00316},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	file = {Preprint PDF:/Users/Roy/Zotero/storage/CSISSZB2/Yang 等 - 2024 - Toward a More Complete OMR Solution.pdf:application/pdf;Snapshot:/Users/Roy/Zotero/storage/AK9GI9UA/2409.html:text/html},
}
