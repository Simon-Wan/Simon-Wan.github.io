---
title: Learning to summarize user information for personalized reinforcement learning
  from human feedback
authors:
- Hyunji Nam
- admin
- Mickel Liu
- Jianxun Lian
- Peter Ahnn
- Natasha Jaques
date: '2025-01-01'
publishDate: '2025-11-10T18:31:32.892071Z'
publication_types:
- paper-conference
publication: '*ArXiv preprint*'
abstract: "As everyday use cases of large language model (LLM) AI assistants have
  expanded, it is becoming increasingly important to personalize responses to align
  to different users' preferences and goals. While reinforcement learning from human
  feedback (RLHF) is effective at improving LLMs to be generally more helpful and
  fluent, it does not account for variability across users, as it models the entire
  user population with a single reward model, meaning it assumes that everyone's preferences
  are the same. We present a novel framework, Preference Learning Using Summarization
  (PLUS), that uses reinforcement learning (RL) to learn to produce text-based summaries
  of each user's preferences, characteristics, and past conversations. These summaries
  condition the reward model, enabling it to make personalized predictions about the
  types of responses valued by each user. Both the user-summarization model and reward
  model are trained simultaneously, creating an online co-adaptation loop. We show
  that in contrast to the standard Bradley-Terry model, summaries produced by PLUS
  capture diverse aspects of user preferences, achieving a 11-77% improvement in reward
  model accuracy. Key strengths of PLUS are: (1) robust performance with new users
  and conversation topics, achieving a 25% improvement over the best personalized
  RLHF technique; (2) zero-shot personalization with state-of-the-art proprietary
  models like GPT-4 (e.g., PLUS-summary-conditioned responses achieved a 72% win rate
  compared to 28% for default GPT-4o); (3) learning from flexible user contexts beyond
  preference labels, and (4) interpretable representation of users, enabling greater
  transparency and user control in pluralistic LLM alignment."
url_pdf: http://arxiv.org/abs/2507.13579
featured: false
---
